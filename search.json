[
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "def answer(text):\n    words = text.strip().split()\n    parts = [ w[-1] for w in words ]\n    return ''.join(parts)\n\n\nanswer('DOG CATTT BANANA')\n\n'GTA'"
  },
  {
    "objectID": "posts/010_More_Puzzles/index.html",
    "href": "posts/010_More_Puzzles/index.html",
    "title": "Overview of MOE",
    "section": "",
    "text": "In a previous blog post I explored how Chat GPT did with a couple of riddles and puzzles from :https://puzzlewocky.com/brain-teasers/classic-short-brain-teasers-and-riddles/. Having always loved puzzles - I thought it would be interesting to explore this a bit more.\nIt is interesting to observe what types of puzzles GPT is able to handle well and what types it can’t.\n\n\nQuestion: David’s father has three sons: Snap, Crackle, and ________?\nGPT Response: David!\nAnswer: David.\nAnalysis: GPT got it right - we can try some harder ones.\n\n\n\nQuestion: George shows Vincent three closed boxes. One contains only apples, one contains only oranges, and one contains a mix of apples and oranges. The boxes are labeled Apples, Oranges, and Mixed, but George says all of the boxes are labeled incorrectly. Vincent is allowed to choose one box and pick one piece of fruit from it at random. He does so and is able to correctly identify all three boxes. How did he do it?\nGPT Response: Vincent knows that all the labels are incorrect, so he can use logic to deduce the correct labels. Here’s how he does it:\nChoose the “Mixed” box: Since every label is wrong, the box labeled “Mixed” cannot actually contain both apples and oranges—it must contain only apples or only oranges.\nPick a fruit from the “Mixed” box: Suppose Vincent picks an apple. That means this box only contains apples (because it cannot be mixed).\nRelabel the boxes logically:\nThe box labeled “Apples” cannot contain only apples (since its label is wrong). It must contain either only oranges or the mixed fruit. The box labeled “Oranges” cannot contain only oranges (since its label is wrong). It must contain either only apples or the mixed fruit.\nSince we now know the “Mixed” box actually contains only apples, the box labeled “Oranges” must be the mixed box (because it cannot actually be oranges-only).\nThat leaves the box labeled “Apples,” which must actually contain only oranges.\nFinal Box Assignments: The “Mixed” box (incorrectly labeled) actually contains only apples. The “Oranges” box (incorrectly labeled) actually contains both apples and oranges. The “Apples” box (incorrectly labeled) actually contains only oranges. By picking just one fruit, Vincent was able to deduce the correct labeling of all the boxes!\nAnswer: Vincent chooses a fruit from the box labeled Mixed. He knows this box is labeled incorrectly, so the box must actually contain either all apples or all oranges. If he pulls out an apple, then that must be the box of all apples. Since the other two boxes are labeled Apples and Oranges, and they are both labeled incorrectly, the box of all oranges must be labeled Apples. That means the box labeled Oranges contains mixed fruit.\nIf he pulls out an orange, then that must be the box of all oranges. Since the other two boxes are labeled Apples and Oranges, and they are both labeled incorrectly, the box of all apples must be labeled Oranges. That means the box labeled Apples contains mixed fruit.\nAnalysis: GPT again gave the correct assessment in a concise way - I think that it was able to break down the problem into a clear concise way to approach the problem.\n\n\n\nQuestion: Erika forgot to wind the clock on her wall, so it has stopped, and that is her only timepiece. She walks to a friend’s house to visit, observing the correct time on her friend’s clock while she is there, and then walks home. When she gets home she makes a simple calculation and then sets her clock to the correct time, despite not knowing how long it took her to walk home from her friend’s house. How did she do it?\nGPT Response: Erika uses a clever trick to determine the correct time without knowing her exact walking duration. Here’s how she does it:\n\nNote the stopped clock’s time before leaving her house.\nWalk to her friend’s house and check the correct time on her friend’s clock upon arrival.\nSpend some time at her friend’s house and check the correct time again just before leaving.\nWalk home and look at the time on the stopped clock upon arrival.\nCalculate the correct time using symmetry:\n\nLet’s say when she left home, the stopped clock showed X.\nWhen she arrived at her friend’s house, the correct time was A.\nWhen she left her friend’s house, the correct time was B.\nWhen she got home, the stopped clock still showed X.\n\n\nSince her walk to and from her friend’s house likely took the same amount of time, the correct time upon her arrival home is the midpoint between A and B:\n[ = B + ]\nBy setting her clock to that calculated time, she correctly resets it!\nAnswer: Before she left her house, Erika wound her clock and set it to 12 o’clock. Therefore, when she returned, she could easily see the total amount of time she had been gone. At her friend’s house, she noted the time when she arrived and when she left, so she knows the amount of time she spent at her friend’s house. She subtracts that from the total time she was gone and divides by two. That is her one-way travel time from her friend’s house, which she adds to the time that was showing on her friend’s clock when she left her friend’s house.\nAnalysis: Again there was a logical thought process - ChatGPT was able to talk through the problem through to the correct answer.\n\n\n\nQuestion: A bat and a ball cost $1.10 together. The bat costs $1 more than the ball. How much does the ball cost?\nGPT Response: Let the cost of the ball be x dollars.\nSince the bat costs $1 more than the ball, the bat’s cost is x + 1 dollars.\nTogether, they cost $1.10, so we set up the equation:\n[ x + (x + 1) = 1.10 ]\nSimplify:\n[ 2x + 1 = 1.10 ]\nSubtract 1 from both sides:\n[ 2x = 0.10 ]\nDivide by 2:\n[ x = 0.05 ]\nSo, the ball costs $0.05 (5 cents), and the bat costs $1.05 (1 dollar and 5 cents), which correctly adds up to $1.10.\nAnswer: The ball costs 5 cents. The bat costs $1.05. Many people answer too quickly and say that the ball costs 10 cents. However, if that were true, and the bat costs $1 more than the ball, then the bat would have to cost $1.10, making the total cost $1.20.\nAnalysis: Here it was interesting to see that ChatGPT took an algebraic approach - that is to say one where it treated the problem like a Math word problem (which it was).\n\n\n\nQuestion: What is unique about the word Alaska?\nGPT Response: The word “Alaska” has several unique features:\n\nOnly U.S. state name with three vowels at the start – It starts with A-L-A, three vowels in a row.\n\nSymmetry in letters – The structure A-L-A-S-K-A has a balanced, almost palindrome-like appearance.\n\nSingle continuous sound – When spoken, it flows smoothly without abrupt breaks.\n\nDerived from an Indigenous word – It comes from the Aleut word Alyeska, meaning “great land.”\n\nContains no repeated consonants – Even though “A” appears three times, all consonants (L, S, K) are unique.\n\nIt’s a visually and phonetically interesting word!\nAnswer: It is the only state name that can be typed using just one row of keys on a keyboard.\nAnalysis: Here GPT got the question wrong and clearly was grasping at straws. That being said there seemed to be a bit of logic in the sense that ackowledging the AKA and trying to break down the letter structure was definitley in the right direction. I think that the hard part about this question was there were was not straight forward and did not provide the needed context. This is indicative of the types of abstract problems that GPT may have issues solving in the future."
  },
  {
    "objectID": "posts/010_More_Puzzles/index.html#more-puzzles",
    "href": "posts/010_More_Puzzles/index.html#more-puzzles",
    "title": "Overview of MOE",
    "section": "",
    "text": "In a previous blog post I explored how Chat GPT did with a couple of riddles and puzzles from :https://puzzlewocky.com/brain-teasers/classic-short-brain-teasers-and-riddles/. Having always loved puzzles - I thought it would be interesting to explore this a bit more.\nIt is interesting to observe what types of puzzles GPT is able to handle well and what types it can’t.\n\n\nQuestion: David’s father has three sons: Snap, Crackle, and ________?\nGPT Response: David!\nAnswer: David.\nAnalysis: GPT got it right - we can try some harder ones.\n\n\n\nQuestion: George shows Vincent three closed boxes. One contains only apples, one contains only oranges, and one contains a mix of apples and oranges. The boxes are labeled Apples, Oranges, and Mixed, but George says all of the boxes are labeled incorrectly. Vincent is allowed to choose one box and pick one piece of fruit from it at random. He does so and is able to correctly identify all three boxes. How did he do it?\nGPT Response: Vincent knows that all the labels are incorrect, so he can use logic to deduce the correct labels. Here’s how he does it:\nChoose the “Mixed” box: Since every label is wrong, the box labeled “Mixed” cannot actually contain both apples and oranges—it must contain only apples or only oranges.\nPick a fruit from the “Mixed” box: Suppose Vincent picks an apple. That means this box only contains apples (because it cannot be mixed).\nRelabel the boxes logically:\nThe box labeled “Apples” cannot contain only apples (since its label is wrong). It must contain either only oranges or the mixed fruit. The box labeled “Oranges” cannot contain only oranges (since its label is wrong). It must contain either only apples or the mixed fruit.\nSince we now know the “Mixed” box actually contains only apples, the box labeled “Oranges” must be the mixed box (because it cannot actually be oranges-only).\nThat leaves the box labeled “Apples,” which must actually contain only oranges.\nFinal Box Assignments: The “Mixed” box (incorrectly labeled) actually contains only apples. The “Oranges” box (incorrectly labeled) actually contains both apples and oranges. The “Apples” box (incorrectly labeled) actually contains only oranges. By picking just one fruit, Vincent was able to deduce the correct labeling of all the boxes!\nAnswer: Vincent chooses a fruit from the box labeled Mixed. He knows this box is labeled incorrectly, so the box must actually contain either all apples or all oranges. If he pulls out an apple, then that must be the box of all apples. Since the other two boxes are labeled Apples and Oranges, and they are both labeled incorrectly, the box of all oranges must be labeled Apples. That means the box labeled Oranges contains mixed fruit.\nIf he pulls out an orange, then that must be the box of all oranges. Since the other two boxes are labeled Apples and Oranges, and they are both labeled incorrectly, the box of all apples must be labeled Oranges. That means the box labeled Apples contains mixed fruit.\nAnalysis: GPT again gave the correct assessment in a concise way - I think that it was able to break down the problem into a clear concise way to approach the problem.\n\n\n\nQuestion: Erika forgot to wind the clock on her wall, so it has stopped, and that is her only timepiece. She walks to a friend’s house to visit, observing the correct time on her friend’s clock while she is there, and then walks home. When she gets home she makes a simple calculation and then sets her clock to the correct time, despite not knowing how long it took her to walk home from her friend’s house. How did she do it?\nGPT Response: Erika uses a clever trick to determine the correct time without knowing her exact walking duration. Here’s how she does it:\n\nNote the stopped clock’s time before leaving her house.\nWalk to her friend’s house and check the correct time on her friend’s clock upon arrival.\nSpend some time at her friend’s house and check the correct time again just before leaving.\nWalk home and look at the time on the stopped clock upon arrival.\nCalculate the correct time using symmetry:\n\nLet’s say when she left home, the stopped clock showed X.\nWhen she arrived at her friend’s house, the correct time was A.\nWhen she left her friend’s house, the correct time was B.\nWhen she got home, the stopped clock still showed X.\n\n\nSince her walk to and from her friend’s house likely took the same amount of time, the correct time upon her arrival home is the midpoint between A and B:\n[ = B + ]\nBy setting her clock to that calculated time, she correctly resets it!\nAnswer: Before she left her house, Erika wound her clock and set it to 12 o’clock. Therefore, when she returned, she could easily see the total amount of time she had been gone. At her friend’s house, she noted the time when she arrived and when she left, so she knows the amount of time she spent at her friend’s house. She subtracts that from the total time she was gone and divides by two. That is her one-way travel time from her friend’s house, which she adds to the time that was showing on her friend’s clock when she left her friend’s house.\nAnalysis: Again there was a logical thought process - ChatGPT was able to talk through the problem through to the correct answer.\n\n\n\nQuestion: A bat and a ball cost $1.10 together. The bat costs $1 more than the ball. How much does the ball cost?\nGPT Response: Let the cost of the ball be x dollars.\nSince the bat costs $1 more than the ball, the bat’s cost is x + 1 dollars.\nTogether, they cost $1.10, so we set up the equation:\n[ x + (x + 1) = 1.10 ]\nSimplify:\n[ 2x + 1 = 1.10 ]\nSubtract 1 from both sides:\n[ 2x = 0.10 ]\nDivide by 2:\n[ x = 0.05 ]\nSo, the ball costs $0.05 (5 cents), and the bat costs $1.05 (1 dollar and 5 cents), which correctly adds up to $1.10.\nAnswer: The ball costs 5 cents. The bat costs $1.05. Many people answer too quickly and say that the ball costs 10 cents. However, if that were true, and the bat costs $1 more than the ball, then the bat would have to cost $1.10, making the total cost $1.20.\nAnalysis: Here it was interesting to see that ChatGPT took an algebraic approach - that is to say one where it treated the problem like a Math word problem (which it was).\n\n\n\nQuestion: What is unique about the word Alaska?\nGPT Response: The word “Alaska” has several unique features:\n\nOnly U.S. state name with three vowels at the start – It starts with A-L-A, three vowels in a row.\n\nSymmetry in letters – The structure A-L-A-S-K-A has a balanced, almost palindrome-like appearance.\n\nSingle continuous sound – When spoken, it flows smoothly without abrupt breaks.\n\nDerived from an Indigenous word – It comes from the Aleut word Alyeska, meaning “great land.”\n\nContains no repeated consonants – Even though “A” appears three times, all consonants (L, S, K) are unique.\n\nIt’s a visually and phonetically interesting word!\nAnswer: It is the only state name that can be typed using just one row of keys on a keyboard.\nAnalysis: Here GPT got the question wrong and clearly was grasping at straws. That being said there seemed to be a bit of logic in the sense that ackowledging the AKA and trying to break down the letter structure was definitley in the right direction. I think that the hard part about this question was there were was not straight forward and did not provide the needed context. This is indicative of the types of abstract problems that GPT may have issues solving in the future."
  },
  {
    "objectID": "posts/004_LLM_Art/index.html",
    "href": "posts/004_LLM_Art/index.html",
    "title": "LLM’s Drawing Words",
    "section": "",
    "text": "One of the areas that I have noticed LLMs to be particularly weak in is specific image generation. These diffusion models perform great when given an abstract task like “draw a cat in the middle of space”:\n\nWe can clearly see that it creatively comes up with a cat (an American cat at that) in vivid detail. We could specify more instruction, like a different art form or change a detail of the cat. However this is where LLMs perfomance in image generation seem to drop off. When tasked with creating more finite images that are seemingly simple - I have found that LLMs tend to create blurry images with mistakes. We can explore this poor perfomance by asking Dalle to generate images containing words that are increasingly complex."
  },
  {
    "objectID": "posts/004_LLM_Art/index.html#image-generation",
    "href": "posts/004_LLM_Art/index.html#image-generation",
    "title": "LLM’s Drawing Words",
    "section": "",
    "text": "One of the areas that I have noticed LLMs to be particularly weak in is specific image generation. These diffusion models perform great when given an abstract task like “draw a cat in the middle of space”:\n\nWe can clearly see that it creatively comes up with a cat (an American cat at that) in vivid detail. We could specify more instruction, like a different art form or change a detail of the cat. However this is where LLMs perfomance in image generation seem to drop off. When tasked with creating more finite images that are seemingly simple - I have found that LLMs tend to create blurry images with mistakes. We can explore this poor perfomance by asking Dalle to generate images containing words that are increasingly complex."
  },
  {
    "objectID": "posts/004_LLM_Art/index.html#generate-an-image-with-the-word-cat",
    "href": "posts/004_LLM_Art/index.html#generate-an-image-with-the-word-cat",
    "title": "LLM’s Drawing Words",
    "section": "“Generate an image with the word CAT”",
    "text": "“Generate an image with the word CAT”\n\nWhile this is techincally correct, it seems to have jumped to doing an artistic rendering of the word CAT. We can try to specify things a bit more to recieve the desired output."
  },
  {
    "objectID": "posts/004_LLM_Art/index.html#generate-a-plain-image-with-the-word-cat-the-word-should-be-black-with-a-simple-white-background",
    "href": "posts/004_LLM_Art/index.html#generate-a-plain-image-with-the-word-cat-the-word-should-be-black-with-a-simple-white-background",
    "title": "LLM’s Drawing Words",
    "section": "“Generate a plain image with the word CAT (the word should be black with a simple white background)”",
    "text": "“Generate a plain image with the word CAT (the word should be black with a simple white background)”\n\nHere we were able to obtain the black word - but the rest of the smaller letters beneath the word CAT look like an eye exam. It is also important to note that some of those smaller letters seem to be smudged or cut off. At this point, maybe if we give some more specification around what we want, we can get the proper result."
  },
  {
    "objectID": "posts/004_LLM_Art/index.html#generate-a-plain-image-with-the-word-cat-the-word-should-be-black-with-a-simple-white-background.-there-should-be-no-other-letters-in-the-image.-the-goal-is-to-mimic-a-black-piece-of-paper-only-the-word-cat.-you-are-not-an-artist.",
    "href": "posts/004_LLM_Art/index.html#generate-a-plain-image-with-the-word-cat-the-word-should-be-black-with-a-simple-white-background.-there-should-be-no-other-letters-in-the-image.-the-goal-is-to-mimic-a-black-piece-of-paper-only-the-word-cat.-you-are-not-an-artist.",
    "title": "LLM’s Drawing Words",
    "section": "“Generate a plain image with the word CAT (the word should be black with a simple white background). There should be no other letters in the image. The goal is to mimic a black piece of paper only the word CAT. You are not an artist.”",
    "text": "“Generate a plain image with the word CAT (the word should be black with a simple white background). There should be no other letters in the image. The goal is to mimic a black piece of paper only the word CAT. You are not an artist.”\n\nThis time around we can see that this is far closer to the result that we want, but it isn’t exactly right. There is an odd grey boarder that was not requested. At this point - maybe we should ask Chat GPT 4o what the best image generation prompt is:\n\nLet’s try it:"
  },
  {
    "objectID": "posts/004_LLM_Art/index.html#a-plain-white-background-with-the-word-cat-in-bold-black-text-centered.-no-other-elements-decorations-or-details.",
    "href": "posts/004_LLM_Art/index.html#a-plain-white-background-with-the-word-cat-in-bold-black-text-centered.-no-other-elements-decorations-or-details.",
    "title": "LLM’s Drawing Words",
    "section": "“A plain white background with the word ‘CAT’ in bold black text, centered. No other elements, decorations, or details.”",
    "text": "“A plain white background with the word ‘CAT’ in bold black text, centered. No other elements, decorations, or details.”\n\nThis did the trick! It seems that the exact prompting with Dalle matters the most. At this point it begs more of a UI question with AI that if text to image if meant to be widely accisble, how can an image generation interface be better suited for a layman user? Maybe it would be better to provide users with a more guided experience, such as preset templates for common requests or a more interactive prompt refinement system - instead of requiring users to perfectly craft a text prompt.\nIn the future when trying to generate an image, it might be best to just ask an LLM what the best way to do this is!"
  },
  {
    "objectID": "posts/002_Memory_Test/index.html",
    "href": "posts/002_Memory_Test/index.html",
    "title": "Memory Test",
    "section": "",
    "text": "I am curious how long LLMs can hold onto user input.\nTo test this I will tell the LLM my favorite color (in this case dark green). Then I will tell it other information about myself like my age, favorite food, etc… before again asking the LLM what my favorite color is. I am going to increase the number of facts about myself to see where it will cap out and forget that my favorite color is dark green. I recognize that part of this constraint is the number of tokens I provide an LLM to remember (that is to say that a 100 word prompt in between would likely have more of an impact than a five word prompt), however for the purpose of this experiment I will only be counting based on the number of “facts” that I provide the LLM.\nThis relevant as when asking simple questions most LLMs can provide a response in the context of their trianing, but one of the more powerful use cases is when it can easily interact wiht user provided information. This is what makes it so powerful when drafting documents or code.\n\n\nGPT4o - 3 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 3 facts.\nGPT4o - 6 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 3 facts.\nGPT4o - 12 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 12 facts. (Note that this is a different chat despite the image continuing).\nGPT4o - 50 Facts\n\nGPT 4o did not remember my favorite color after 50 facts. (Note that this is a different chat despite the image continuing)."
  },
  {
    "objectID": "posts/002_Memory_Test/index.html#memory-test",
    "href": "posts/002_Memory_Test/index.html#memory-test",
    "title": "Memory Test",
    "section": "",
    "text": "I am curious how long LLMs can hold onto user input.\nTo test this I will tell the LLM my favorite color (in this case dark green). Then I will tell it other information about myself like my age, favorite food, etc… before again asking the LLM what my favorite color is. I am going to increase the number of facts about myself to see where it will cap out and forget that my favorite color is dark green. I recognize that part of this constraint is the number of tokens I provide an LLM to remember (that is to say that a 100 word prompt in between would likely have more of an impact than a five word prompt), however for the purpose of this experiment I will only be counting based on the number of “facts” that I provide the LLM.\nThis relevant as when asking simple questions most LLMs can provide a response in the context of their trianing, but one of the more powerful use cases is when it can easily interact wiht user provided information. This is what makes it so powerful when drafting documents or code.\n\n\nGPT4o - 3 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 3 facts.\nGPT4o - 6 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 3 facts.\nGPT4o - 12 Facts\n\nGPT 4o correctly remembered that my favorite color was dark green after 12 facts. (Note that this is a different chat despite the image continuing).\nGPT4o - 50 Facts\n\nGPT 4o did not remember my favorite color after 50 facts. (Note that this is a different chat despite the image continuing)."
  },
  {
    "objectID": "posts/002_Memory_Test/index.html#takeaways",
    "href": "posts/002_Memory_Test/index.html#takeaways",
    "title": "Memory Test",
    "section": "Takeaways",
    "text": "Takeaways\nIt is worth noting that initially GPT did a great job at remembering this simple fact after preventing it with a few number of facts, but as that number grew, at some point it was not able to remeber the intial piece of information. My understanding is that this is because there is memory that GPT uses that will run out at a certain point. I would also imagine (as noted above) that the length of the input and information GPT has to remember would impact how many messages can pass while GPT retains this information - which at a certain point is system design more than anything else."
  },
  {
    "objectID": "posts/001_testing_again-Copy1-Copy1-Copy1/index.html",
    "href": "posts/001_testing_again-Copy1-Copy1-Copy1/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/001_testing_again-Copy1-Copy1-Copy1/index.html#a-visualization-problem-for-llms",
    "href": "posts/001_testing_again-Copy1-Copy1-Copy1/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "TESTING AGAIN",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "TESTING AGAIN",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Memory Test\n\n\n\n\n\n\nLLMs\n\n\nMemory\n\n\n\nTesting how long an LLM can remeber my favorite color\n\n\n\n\n\nFeb 3, 2025\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nTESTING AGAIN\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nJan 29, 2025\n\n\nLOGAN BRASSINGTON\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of MOE\n\n\n\n\n\n\nLLMs\n\n\nModels\n\n\n\nAn overview of MOE (Mixture of Experts)\n\n\n\n\n\nMar 1, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nClaude 3.7 is released\n\n\n\n\n\n\nLLMs\n\n\nClaude\n\n\n\nExploration of new model\n\n\n\n\n\nFeb 25, 2024\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nGPT Solving Puzzles\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\npuzzles\n\n\n\nCan GPT solve some classic brain teasers?\n\n\n\n\n\nFeb 23, 2024\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nAI Art Refinement\n\n\n\n\n\n\nLLMs\n\n\nArt\n\n\nPrompting\n\n\n\nCan Dalle refine Images properlyy with natural language?\n\n\n\n\n\nFeb 20, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT Replace a Counsler?\n\n\n\n\n\n\nLLMs\n\n\nJob Market\n\n\n\nExploration into LLM for job replacement\n\n\n\n\n\nFeb 18, 2024\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nLLM’s Drawing Words\n\n\n\n\n\n\nLLMs\n\n\nArt\n\n\n\nExploration of \n\n\n\n\n\nFeb 10, 2024\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nHumanity’s Last Exam Overview\n\n\n\n\n\n\nLLMs\n\n\nBenchmarking\n\n\n\nAn overview of the HLE Benchmark and how GPT 4o does on a few questions\n\n\n\n\n\nFeb 6, 2024\n\n\nLogan Brassington\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/009_Misinformation/index.html",
    "href": "posts/009_Misinformation/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/009_Misinformation/index.html#a-visualization-problem-for-llms",
    "href": "posts/009_Misinformation/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/005_ChatGPT_Emotional_Response/index.html",
    "href": "posts/005_ChatGPT_Emotional_Response/index.html",
    "title": "Can ChatGPT Replace a Counsler?",
    "section": "",
    "text": "One of the fears that many have is that AI will take their job. They fear that one day we will wake up and GenAI will be able to take the job of every Software Engineer, Accountant, Business Analyst - anything with code of a spreadsheet. However some feel that they will not be touched at all by the AI revolution - Postmen, Plumbers, Construction workers - anyone who works in the “real world”. There are others who have confidence but should perhaps be less sure - like Psychologists or Counslers. Today I will explore how effective Chat GPT 4o is at when I share that my mental health is not great (something that I will note is purely for expirmentation purposes.\n\n\nBelow you will see that I: - Share my inital thoughts and feelings - Respond to any questions / provide more detail if needed - Then ask what I should do going forward\nI am curious if ChatGPT will try to tackle the issue head on on or if GPT will recomend professional (non-AI) help at any point."
  },
  {
    "objectID": "posts/005_ChatGPT_Emotional_Response/index.html#which-jobs-will-llms-replace",
    "href": "posts/005_ChatGPT_Emotional_Response/index.html#which-jobs-will-llms-replace",
    "title": "Can ChatGPT Replace a Counsler?",
    "section": "",
    "text": "One of the fears that many have is that AI will take their job. They fear that one day we will wake up and GenAI will be able to take the job of every Software Engineer, Accountant, Business Analyst - anything with code of a spreadsheet. However some feel that they will not be touched at all by the AI revolution - Postmen, Plumbers, Construction workers - anyone who works in the “real world”. There are others who have confidence but should perhaps be less sure - like Psychologists or Counslers. Today I will explore how effective Chat GPT 4o is at when I share that my mental health is not great (something that I will note is purely for expirmentation purposes.\n\n\nBelow you will see that I: - Share my inital thoughts and feelings - Respond to any questions / provide more detail if needed - Then ask what I should do going forward\nI am curious if ChatGPT will try to tackle the issue head on on or if GPT will recomend professional (non-AI) help at any point."
  },
  {
    "objectID": "posts/005_ChatGPT_Emotional_Response/index.html#conversation",
    "href": "posts/005_ChatGPT_Emotional_Response/index.html#conversation",
    "title": "Can ChatGPT Replace a Counsler?",
    "section": "Conversation",
    "text": "Conversation\nYou said: Hi! I am not feeling so great today.\nChatGPT said: Hey Logan, I’m sorry to hear that. Rough day, or just not feeling 100%? Want to talk about it, or would you rather just chat about something fun to take your mind off things?\nYou said: I’ll talk about it a bit - I just had a really tough day, there was so much going on between classes, work, and dealing with others.\nChatGPT said: That sounds really overwhelming, Logan. Juggling classes, work, and dealing with people all at once can be exhausting. I can imagine it feels like there’s barely any room to breathe.\nWas there something specific that made today extra tough, or was it just everything piling up at once?\nYou said: I think it was just everything piling up at once.\nChatGPT said: Yeah, that makes sense. When everything stacks up like that, even small things can feel way heavier. It’s frustrating when there’s no clear break to catch your breath.\nDo you have anything coming up—like a free evening, a small break, or even just a quiet moment—where you can take a step back and reset? Or do you feel like the pressure’s still going to keep building?\nYou said: Yeah I will hopefully have a break during the weekend - then maybe something fun will happen.\nChatGPT said: That’s good to hear—you definitely deserve a break. Maybe you can plan something small to look forward to, even if it’s just a chill night, watching a movie, or doing something you enjoy.\nIf you want, we can brainstorm something fun to do this weekend, or I can just be here to chat and take your mind off things for a bit. Whatever helps!\nYou said: What ideas do you have - how can avoid this in the future?\nChatGPT said: For the weekend, it depends on what kind of break you need. Here are a few ideas based on different ways you might want to recharge:\n\nLow-effort relaxation: Watch a movie, play a casual game, or go for a walk somewhere nice. Something that doesn’t require much thinking but lets you reset.\nSocial but stress-free: Hang out with friends in a way that doesn’t add pressure—maybe a board game night, getting food, or just vibing somewhere.\nSomething new/exciting: If you want to shake things up, you could try a new activity—an escape room, a new restaurant, or a random event happening near you.\nPhysical reset: If you feel burnt out, something like going to the gym, playing a sport, or even just stretching and getting fresh air could help.\n\nHow to Avoid This in the Future It’s tough, but here are a few ways to keep things from piling up too much:\n\nPreemptively block off time for breaks – Even if you’re super busy, scheduling small breaks (even 10-15 mins) can stop things from spiraling.\nSet ‘off-limits’ hours – A time at night where you stop working and let your mind rest. No school, no work, just chill time. Prioritize what actually matters – When everything feels urgent, it’s easy to stress over things that don’t deserve it. If you can, let go of the smaller stuff.\nDelegate or push back where possible – If you’re taking on too much at once, is there anything you can shift around or ask for help with?\nCheck in with yourself sooner – If you feel stress building mid-week, take a step back instead of letting it pile up to breaking point.\nWhat do you think? Do any of these feel doable, or is there something specific that usually pushes you over the edge?\n\nYou said: Yes this all helps! Thank you so much!\nChatGPT said: I’m really glad to hear that it helped! It sounds like you’re finding some good ways to take a breather, and I hope your weekend break brings you the recharge you need. Remember, it’s totally okay to prioritize yourself when things get overwhelming. If you ever want to chat more or brainstorm additional ideas, I’m here for you. Take care!\n\nTakeways\nOverall ChatGPT did a solid job at providing something to talk to and recenter my attention. I did find it interesting that it kept throwing things back onto me to push the conversation forward - I had to prompt it to give specific advice. FWIW - I think that LLMS perform better as you give more information - maybe I should have selected a more specific situation.\nI found it interesting that ChatGPT did not recomend professional help at any point. I think that there is great promise for the ChatGPT to be a mental health first responder of sorts - especially as LLMs are applied to voices / humanoids to be more personal."
  },
  {
    "objectID": "posts/006_adjusting_art/index.html",
    "href": "posts/006_adjusting_art/index.html",
    "title": "AI Art Refinement",
    "section": "",
    "text": "One of the things that I am curious about - is if LLMs can generate images based on natural language then make small modificaitons without losing the overall image. I will try a variety of tests to see how DALLE performs with making a varitey of shifts to an inital “crazy” prompt.\n\n\nThe inital prompt will intentially be something fun and crazy - the type of image generation that LLMs do well. “Generate an image of a cat riding a bike on Mars”\n\n\n\n\nThis did a great job comeing up with something abstract - let’s see if they can keep it exactly the same except have the cat be black. “This looks great! Can you keep the image EXACTLY the same - but make the cat black?”\n\n\n\n\nThe generated image has a number of differences - most notably the the planents in the background shift - the sky is different - as is the texture of Mars. Let’s try one quick correction to see if we can get a better result. “This is not the same as the first image. Make exactly the first image - except have the cat be black!”\n\n\n\n\nThis was a good attempt - GPT acknowledged that it cannot directly recreate the image but it can make a similar type. Curious about why the model works this way - I asked GPT why it cannot edit the image directly. “Why can you not edit the image directly?”\n\nEver helpful - GPT offers alternatives to make up for the fact that it cannot directly recreate the image.\nAs an aside I did come across a tool called Control Net (https://github.com/lllyasviel/ControlNet) - which is an LLM wrapper that provides the ability to further control existing diffusion models - But more on that later."
  },
  {
    "objectID": "posts/006_adjusting_art/index.html#art-generation-for-llms",
    "href": "posts/006_adjusting_art/index.html#art-generation-for-llms",
    "title": "AI Art Refinement",
    "section": "",
    "text": "One of the things that I am curious about - is if LLMs can generate images based on natural language then make small modificaitons without losing the overall image. I will try a variety of tests to see how DALLE performs with making a varitey of shifts to an inital “crazy” prompt.\n\n\nThe inital prompt will intentially be something fun and crazy - the type of image generation that LLMs do well. “Generate an image of a cat riding a bike on Mars”\n\n\n\n\nThis did a great job comeing up with something abstract - let’s see if they can keep it exactly the same except have the cat be black. “This looks great! Can you keep the image EXACTLY the same - but make the cat black?”\n\n\n\n\nThe generated image has a number of differences - most notably the the planents in the background shift - the sky is different - as is the texture of Mars. Let’s try one quick correction to see if we can get a better result. “This is not the same as the first image. Make exactly the first image - except have the cat be black!”\n\n\n\n\nThis was a good attempt - GPT acknowledged that it cannot directly recreate the image but it can make a similar type. Curious about why the model works this way - I asked GPT why it cannot edit the image directly. “Why can you not edit the image directly?”\n\nEver helpful - GPT offers alternatives to make up for the fact that it cannot directly recreate the image.\nAs an aside I did come across a tool called Control Net (https://github.com/lllyasviel/ControlNet) - which is an LLM wrapper that provides the ability to further control existing diffusion models - But more on that later."
  },
  {
    "objectID": "posts/008_Claude_37/index.html",
    "href": "posts/008_Claude_37/index.html",
    "title": "Claude 3.7 is released",
    "section": "",
    "text": "This past week Anthropic announced Claude 3.7 Sonnet. Anthropic largely leads the LLM world for generating code that seemlessly fits into software. In it’s release Anthropic touted Claude 3.7 to be made with a different Philosophy - with an extended thinking mode that can be enabled to fit a given task. The release stated, “In extended thinking mode, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks.” One of the interesting features that Claude came up with for this iteration is that users of their API can assign a budget to a given task - a differentiator with the space as many users complain of LLMs chewing through their credit.\n\n\nBelow we can see the performance of the 3.7 modes on different benchmarks. I think it is important to note the difference on AIME - the highschool math competition. The extended thinking performed quite better, than the non-extended thinking version. This is important because this exam is indicative of the types of problems that Computer Scientists must generally solve and informs the models performance with more complicated algorithms.\n\n\n\n\nTo illustrate the difference between these two types that Anthropic just released - I am going to pose the same prompt to the different versions and record the responses.\nThe prompt is going to be “Code the general logic for the game 2048”.\n\n\n\nI will note that Claude “thought” for about 10 seconds before generation this response and code:\nI’ll create the core logic for the 2048 game in JavaScript. This implementation will include the game board, movement mechanics, tile merging, and win/lose conditions.\nThis implementation provides all the core game logic for 2048. Here’s an overview of how it works:\n\nThe game is played on a 4×4 grid\nInitially, two tiles (either 2 or 4) are randomly placed on the grid\nEach move slides all tiles in one direction (up, down, left, or right)\nTiles with the same values merge when they collide during a move\nAfter each successful move, a new tile (2 or 4) appears on a random empty cell\nThe game is won when a tile with the value 2048 is created\nThe game is lost when the grid is full and no more moves are possible\n\nThe code handles: - Moving and merging tiles in all four directions - Tracking the game score (incremented when tiles merge) - Detecting win condition (2048 tile created) - Detecting loss condition (no valid moves possible) - Managing the game state\nTo use this code, you’d need to create a user interface that displays the grid and handles user input. The commented section at the bottom shows an example of how to handle keyboard input.\nWould you like me to explain any specific part of the implementation in more detail?\nHere is the implementation class Game2048 { constructor() { // Initialize 4x4 game board this.grid = Array(4).fill().map(() =&gt; Array(4).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Start the game with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n}\n// Reset the game reset() { this.grid = Array(4).fill().map(() =&gt; Array(4).fill(0)); this.score = 0; this.won = false; this.lost = false; this.addRandomTile(); this.addRandomTile(); }\n// Get all empty cells getEmptyCells() { const emptyCells = []; for (let r = 0; r &lt; 4; r++) { for (let c = 0; c &lt; 4; c++) { if (this.grid[r][c] === 0) { emptyCells.push({ r, c }); } } } return emptyCells; }\n// Add a random tile (2 or 4) to an empty cell addRandomTile() { const emptyCells = this.getEmptyCells();\nif (emptyCells.length &gt; 0) {\n  const { r, c } = emptyCells[Math.floor(Math.random() * emptyCells.length)];\n  // 90% chance of 2, 10% chance of 4\n  this.grid[r][c] = Math.random() &lt; 0.9 ? 2 : 4;\n}\n}\n// Check if the game is over isGameOver() { // Check if board is full if (this.getEmptyCells().length &gt; 0) { return false; }\n// Check if any moves are possible\nfor (let r = 0; r &lt; 4; r++) {\n  for (let c = 0; c &lt; 3; c++) {\n    if (this.grid[r][c] === this.grid[r][c + 1]) {\n      return false;\n    }\n  }\n}\n\nfor (let c = 0; c &lt; 4; c++) {\n  for (let r = 0; r &lt; 3; r++) {\n    if (this.grid[r][c] === this.grid[r + 1][c]) {\n      return false;\n    }\n  }\n}\n\nreturn true;\n}\n// Check if player has won checkWin() { for (let r = 0; r &lt; 4; r++) { for (let c = 0; c &lt; 4; c++) { if (this.grid[r][c] === 2048) { this.won = true; return true; } } } return false; }\n// Move tiles to the left and merge if possible moveLeft() { let moved = false;\nfor (let r = 0; r &lt; 4; r++) {\n  // 1. Remove all zeros\n  let row = this.grid[r].filter(val =&gt; val !== 0);\n  \n  // 2. Merge adjacent matching values\n  for (let c = 0; c &lt; row.length - 1; c++) {\n    if (row[c] === row[c + 1]) {\n      row[c] *= 2;\n      this.score += row[c];\n      row.splice(c + 1, 1);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (row.length &lt; 4) {\n    row.push(0);\n  }\n  \n  // 4. Check if anything moved\n  for (let c = 0; c &lt; 4; c++) {\n    if (this.grid[r][c] !== row[c]) {\n      moved = true;\n    }\n  }\n  \n  // 5. Update the grid\n  this.grid[r] = row;\n}\n\nreturn moved;\n}\n// Move tiles to the right and merge if possible moveRight() { let moved = false;\nfor (let r = 0; r &lt; 4; r++) {\n  // 1. Remove all zeros\n  let row = this.grid[r].filter(val =&gt; val !== 0);\n  \n  // 2. Merge adjacent matching values from right to left\n  for (let c = row.length - 1; c &gt; 0; c--) {\n    if (row[c] === row[c - 1]) {\n      row[c] *= 2;\n      this.score += row[c];\n      row.splice(c - 1, 1);\n      row.unshift(0);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (row.length &lt; 4) {\n    row.unshift(0);\n  }\n  \n  // 4. Check if anything moved\n  for (let c = 0; c &lt; 4; c++) {\n    if (this.grid[r][c] !== row[c]) {\n      moved = true;\n    }\n  }\n  \n  // 5. Update the grid\n  this.grid[r] = row;\n}\n\nreturn moved;\n}\n// Move tiles up and merge if possible moveUp() { let moved = false;\nfor (let c = 0; c &lt; 4; c++) {\n  // 1. Create a column and remove zeros\n  let col = [];\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== 0) {\n      col.push(this.grid[r][c]);\n    }\n  }\n  \n  // 2. Merge adjacent matching values\n  for (let r = 0; r &lt; col.length - 1; r++) {\n    if (col[r] === col[r + 1]) {\n      col[r] *= 2;\n      this.score += col[r];\n      col.splice(r + 1, 1);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (col.length &lt; 4) {\n    col.push(0);\n  }\n  \n  // 4. Update the grid and check if moved\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== col[r]) {\n      moved = true;\n    }\n    this.grid[r][c] = col[r];\n  }\n}\n\nreturn moved;\n}\n// Move tiles down and merge if possible moveDown() { let moved = false;\nfor (let c = 0; c &lt; 4; c++) {\n  // 1. Create a column and remove zeros\n  let col = [];\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== 0) {\n      col.push(this.grid[r][c]);\n    }\n  }\n  \n  // 2. Merge adjacent matching values from bottom to top\n  for (let r = col.length - 1; r &gt; 0; r--) {\n    if (col[r] === col[r - 1]) {\n      col[r] *= 2;\n      this.score += col[r];\n      col.splice(r - 1, 1);\n      col.unshift(0);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (col.length &lt; 4) {\n    col.unshift(0);\n  }\n  \n  // 4. Update the grid and check if moved\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== col[r]) {\n      moved = true;\n    }\n    this.grid[r][c] = col[r];\n  }\n}\n\nreturn moved;\n}\n// Make a move in the specified direction move(direction) { if (this.won || this.lost) { return false; }\nlet moved = false;\n\nswitch (direction) {\n  case 'left':\n    moved = this.moveLeft();\n    break;\n  case 'right':\n    moved = this.moveRight();\n    break;\n  case 'up':\n    moved = this.moveUp();\n    break;\n  case 'down':\n    moved = this.moveDown();\n    break;\n}\n\nif (moved) {\n  this.addRandomTile();\n  this.checkWin();\n  \n  if (this.isGameOver()) {\n    this.lost = true;\n  }\n}\n\nreturn moved;\n}\n// Get the current game state getState() { return { grid: this.grid, score: this.score, won: this.won, lost: this.lost, emptyCells: this.getEmptyCells().length }; } }\n// Example usage: // // const game = new Game2048(); // // // Handle keyboard input // document.addEventListener(‘keydown’, (event) =&gt; { // switch(event.key) { // case ‘ArrowLeft’: // game.move(‘left’); // break; // case ‘ArrowRight’: // game.move(‘right’); // break; // case ‘ArrowUp’: // game.move(‘up’); // break; // case ‘ArrowDown’: // game.move(‘down’); // break; // } //\n// // Update UI based on game.getState() // });\n\n\n\nI’ll create the core logic for the 2048 game. Let me implement that for you:\nThis JavaScript implementation provides the complete logic for the 2048 game. The key aspects of the implementation include:\n\nBoard Management:\n\n4x4 grid represented as a 2D array\nInitialization with two random tiles (90% chance of 2, 10% chance of 4)\n\nGame Mechanics:\n\nMovement in four directions (up, right, down, left)\nMerging identical tiles during movement\nScore tracking when tiles merge\nWin detection when a 2048 tile is created\nLoss detection when no moves are possible\n\nGame Flow:\n\nAfter each valid move, a new random tile appears\nThe game ends when either 2048 is reached (win) or no more moves are possible (loss)\n\n\nTo use this code, you would create a new game instance and call the move method with the direction (“up”, “right”, “down”, or “left”). The getState method returns the current state of the game including the grid, score, and game status.\nWould you like me to explain any specific part of the implementation in more detail?\nBelow is the code implementation\nclass Game2048 { constructor(size = 4) { this.size = size; this.grid = Array(size).fill().map(() =&gt; Array(size).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Initialize with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n}\n// Add a new random tile (2 or 4) to an empty cell addRandomTile() { if (this.isGridFull()) return false;\nlet emptyCells = [];\n\n// Find all empty cells\nfor (let row = 0; row &lt; this.size; row++) {\n  for (let col = 0; col &lt; this.size; col++) {\n    if (this.grid[row][col] === 0) {\n      emptyCells.push({ row, col });\n    }\n  }\n}\n\n// If there are empty cells, add a new tile\nif (emptyCells.length &gt; 0) {\n  const { row, col } = emptyCells[Math.floor(Math.random() * emptyCells.length)];\n  // 90% chance of 2, 10% chance of 4\n  this.grid[row][col] = Math.random() &lt; 0.9 ? 2 : 4;\n  return true;\n}\n\nreturn false;\n}\n// Check if the grid is completely filled isGridFull() { for (let row = 0; row &lt; this.size; row++) { for (let col = 0; col &lt; this.size; col++) { if (this.grid[row][col] === 0) { return false; } } } return true; }\n// Check if any moves are possible canMove() { // Check for empty cells if (!this.isGridFull()) return true;\n// Check for possible merges horizontally and vertically\nfor (let row = 0; row &lt; this.size; row++) {\n  for (let col = 0; col &lt; this.size; col++) {\n    const value = this.grid[row][col];\n    \n    // Check right\n    if (col &lt; this.size - 1 && value === this.grid[row][col + 1]) {\n      return true;\n    }\n    \n    // Check down\n    if (row &lt; this.size - 1 && value === this.grid[row + 1][col]) {\n      return true;\n    }\n  }\n}\n\nreturn false;\n}\n// Move tiles in a specific direction and merge where possible move(direction) { if (this.won || this.lost) return false;\nlet moved = false;\nlet prevGrid = JSON.parse(JSON.stringify(this.grid));\n\nswitch(direction) {\n  case 'up':\n    moved = this.moveUp();\n    break;\n  case 'right':\n    moved = this.moveRight();\n    break;\n  case 'down':\n    moved = this.moveDown();\n    break;\n  case 'left':\n    moved = this.moveLeft();\n    break;\n  default:\n    return false;\n}\n\n// If the grid changed, add a new random tile\nif (moved) {\n  this.addRandomTile();\n  \n  // Check if the player won (reached 2048)\n  this.checkWin();\n  \n  // Check if the player lost (no moves available)\n  if (!this.canMove()) {\n    this.lost = true;\n  }\n  \n  return true;\n}\n\nreturn false;\n}\n// Helper function to rotate the grid for easier move implementation rotateGrid(times = 1) { let newGrid = JSON.parse(JSON.stringify(this.grid));\nfor (let t = 0; t &lt; times; t++) {\n  const rotated = Array(this.size).fill().map(() =&gt; Array(this.size).fill(0));\n  \n  for (let row = 0; row &lt; this.size; row++) {\n    for (let col = 0; col &lt; this.size; col++) {\n      rotated[col][this.size - 1 - row] = newGrid[row][col];\n    }\n  }\n  \n  newGrid = rotated;\n}\n\nreturn newGrid;\n}\n// Move tiles left and merge where possible moveLeft() { let moved = false;\nfor (let row = 0; row &lt; this.size; row++) {\n  // Get non-zero tiles in this row\n  let tiles = this.grid[row].filter(val =&gt; val !== 0);\n  \n  // Merge adjacent equal tiles\n  for (let i = 0; i &lt; tiles.length - 1; i++) {\n    if (tiles[i] === tiles[i + 1]) {\n      tiles[i] *= 2;\n      tiles[i + 1] = 0;\n      this.score += tiles[i];\n      moved = true;\n      i++; // Skip the merged tile\n    }\n  }\n  \n  // Remove zeros (merged tiles)\n  tiles = tiles.filter(val =&gt; val !== 0);\n  \n  // Fill in remaining cells with zeros\n  while (tiles.length &lt; this.size) {\n    tiles.push(0);\n  }\n  \n  // Check if the row changed\n  if (!this.grid[row].every((val, idx) =&gt; val === tiles[idx])) {\n    moved = true;\n  }\n  \n  // Update the row\n  this.grid[row] = tiles;\n}\n\nreturn moved;\n}\n// Move tiles right by reversing, moving left, and reversing back moveRight() { for (let row = 0; row &lt; this.size; row++) { this.grid[row].reverse(); }\nconst moved = this.moveLeft();\n\nfor (let row = 0; row &lt; this.size; row++) {\n  this.grid[row].reverse();\n}\n\nreturn moved;\n}\n// Move tiles up by transposing, moving left, and transposing back moveUp() { // Transpose this.grid = this.rotateGrid(3);\nconst moved = this.moveLeft();\n\n// Transpose back\nthis.grid = this.rotateGrid();\n\nreturn moved;\n}\n// Move tiles down by transposing, moving right, and transposing back moveDown() { // Transpose this.grid = this.rotateGrid(3);\nconst moved = this.moveRight();\n\n// Transpose back\nthis.grid = this.rotateGrid();\n\nreturn moved;\n}\n// Check if the player has won (reached 2048) checkWin() { for (let row = 0; row &lt; this.size; row++) { for (let col = 0; col &lt; this.size; col++) { if (this.grid[row][col] === 2048) { this.won = true; return true; } } } return false; }\n// Get the current state of the game getState() { return { grid: this.grid, score: this.score, won: this.won, lost: this.lost, canMove: this.canMove() }; }\n// Reset the game reset() { this.grid = Array(this.size).fill().map(() =&gt; Array(this.size).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Initialize with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n} }\n// Example usage: // const game = new Game2048(); // game.move(‘left’); // game.move(‘right’); // game.move(‘up’); // game.move(‘down’); // console.log(game.getState());\n\n\n\nWhile on the surface the two outputs look very similar we can see that the code in the extended version is better commented and goes into greater detail. Perhaps this was not the best test case to see the differences in the models - it will be interesting to see how this gap grows and what types of models are used for everyday tasks."
  },
  {
    "objectID": "posts/008_Claude_37/index.html#claude-3.7-sonnet",
    "href": "posts/008_Claude_37/index.html#claude-3.7-sonnet",
    "title": "Claude 3.7 is released",
    "section": "",
    "text": "This past week Anthropic announced Claude 3.7 Sonnet. Anthropic largely leads the LLM world for generating code that seemlessly fits into software. In it’s release Anthropic touted Claude 3.7 to be made with a different Philosophy - with an extended thinking mode that can be enabled to fit a given task. The release stated, “In extended thinking mode, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks.” One of the interesting features that Claude came up with for this iteration is that users of their API can assign a budget to a given task - a differentiator with the space as many users complain of LLMs chewing through their credit.\n\n\nBelow we can see the performance of the 3.7 modes on different benchmarks. I think it is important to note the difference on AIME - the highschool math competition. The extended thinking performed quite better, than the non-extended thinking version. This is important because this exam is indicative of the types of problems that Computer Scientists must generally solve and informs the models performance with more complicated algorithms.\n\n\n\n\nTo illustrate the difference between these two types that Anthropic just released - I am going to pose the same prompt to the different versions and record the responses.\nThe prompt is going to be “Code the general logic for the game 2048”.\n\n\n\nI will note that Claude “thought” for about 10 seconds before generation this response and code:\nI’ll create the core logic for the 2048 game in JavaScript. This implementation will include the game board, movement mechanics, tile merging, and win/lose conditions.\nThis implementation provides all the core game logic for 2048. Here’s an overview of how it works:\n\nThe game is played on a 4×4 grid\nInitially, two tiles (either 2 or 4) are randomly placed on the grid\nEach move slides all tiles in one direction (up, down, left, or right)\nTiles with the same values merge when they collide during a move\nAfter each successful move, a new tile (2 or 4) appears on a random empty cell\nThe game is won when a tile with the value 2048 is created\nThe game is lost when the grid is full and no more moves are possible\n\nThe code handles: - Moving and merging tiles in all four directions - Tracking the game score (incremented when tiles merge) - Detecting win condition (2048 tile created) - Detecting loss condition (no valid moves possible) - Managing the game state\nTo use this code, you’d need to create a user interface that displays the grid and handles user input. The commented section at the bottom shows an example of how to handle keyboard input.\nWould you like me to explain any specific part of the implementation in more detail?\nHere is the implementation class Game2048 { constructor() { // Initialize 4x4 game board this.grid = Array(4).fill().map(() =&gt; Array(4).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Start the game with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n}\n// Reset the game reset() { this.grid = Array(4).fill().map(() =&gt; Array(4).fill(0)); this.score = 0; this.won = false; this.lost = false; this.addRandomTile(); this.addRandomTile(); }\n// Get all empty cells getEmptyCells() { const emptyCells = []; for (let r = 0; r &lt; 4; r++) { for (let c = 0; c &lt; 4; c++) { if (this.grid[r][c] === 0) { emptyCells.push({ r, c }); } } } return emptyCells; }\n// Add a random tile (2 or 4) to an empty cell addRandomTile() { const emptyCells = this.getEmptyCells();\nif (emptyCells.length &gt; 0) {\n  const { r, c } = emptyCells[Math.floor(Math.random() * emptyCells.length)];\n  // 90% chance of 2, 10% chance of 4\n  this.grid[r][c] = Math.random() &lt; 0.9 ? 2 : 4;\n}\n}\n// Check if the game is over isGameOver() { // Check if board is full if (this.getEmptyCells().length &gt; 0) { return false; }\n// Check if any moves are possible\nfor (let r = 0; r &lt; 4; r++) {\n  for (let c = 0; c &lt; 3; c++) {\n    if (this.grid[r][c] === this.grid[r][c + 1]) {\n      return false;\n    }\n  }\n}\n\nfor (let c = 0; c &lt; 4; c++) {\n  for (let r = 0; r &lt; 3; r++) {\n    if (this.grid[r][c] === this.grid[r + 1][c]) {\n      return false;\n    }\n  }\n}\n\nreturn true;\n}\n// Check if player has won checkWin() { for (let r = 0; r &lt; 4; r++) { for (let c = 0; c &lt; 4; c++) { if (this.grid[r][c] === 2048) { this.won = true; return true; } } } return false; }\n// Move tiles to the left and merge if possible moveLeft() { let moved = false;\nfor (let r = 0; r &lt; 4; r++) {\n  // 1. Remove all zeros\n  let row = this.grid[r].filter(val =&gt; val !== 0);\n  \n  // 2. Merge adjacent matching values\n  for (let c = 0; c &lt; row.length - 1; c++) {\n    if (row[c] === row[c + 1]) {\n      row[c] *= 2;\n      this.score += row[c];\n      row.splice(c + 1, 1);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (row.length &lt; 4) {\n    row.push(0);\n  }\n  \n  // 4. Check if anything moved\n  for (let c = 0; c &lt; 4; c++) {\n    if (this.grid[r][c] !== row[c]) {\n      moved = true;\n    }\n  }\n  \n  // 5. Update the grid\n  this.grid[r] = row;\n}\n\nreturn moved;\n}\n// Move tiles to the right and merge if possible moveRight() { let moved = false;\nfor (let r = 0; r &lt; 4; r++) {\n  // 1. Remove all zeros\n  let row = this.grid[r].filter(val =&gt; val !== 0);\n  \n  // 2. Merge adjacent matching values from right to left\n  for (let c = row.length - 1; c &gt; 0; c--) {\n    if (row[c] === row[c - 1]) {\n      row[c] *= 2;\n      this.score += row[c];\n      row.splice(c - 1, 1);\n      row.unshift(0);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (row.length &lt; 4) {\n    row.unshift(0);\n  }\n  \n  // 4. Check if anything moved\n  for (let c = 0; c &lt; 4; c++) {\n    if (this.grid[r][c] !== row[c]) {\n      moved = true;\n    }\n  }\n  \n  // 5. Update the grid\n  this.grid[r] = row;\n}\n\nreturn moved;\n}\n// Move tiles up and merge if possible moveUp() { let moved = false;\nfor (let c = 0; c &lt; 4; c++) {\n  // 1. Create a column and remove zeros\n  let col = [];\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== 0) {\n      col.push(this.grid[r][c]);\n    }\n  }\n  \n  // 2. Merge adjacent matching values\n  for (let r = 0; r &lt; col.length - 1; r++) {\n    if (col[r] === col[r + 1]) {\n      col[r] *= 2;\n      this.score += col[r];\n      col.splice(r + 1, 1);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (col.length &lt; 4) {\n    col.push(0);\n  }\n  \n  // 4. Update the grid and check if moved\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== col[r]) {\n      moved = true;\n    }\n    this.grid[r][c] = col[r];\n  }\n}\n\nreturn moved;\n}\n// Move tiles down and merge if possible moveDown() { let moved = false;\nfor (let c = 0; c &lt; 4; c++) {\n  // 1. Create a column and remove zeros\n  let col = [];\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== 0) {\n      col.push(this.grid[r][c]);\n    }\n  }\n  \n  // 2. Merge adjacent matching values from bottom to top\n  for (let r = col.length - 1; r &gt; 0; r--) {\n    if (col[r] === col[r - 1]) {\n      col[r] *= 2;\n      this.score += col[r];\n      col.splice(r - 1, 1);\n      col.unshift(0);\n      moved = true;\n    }\n  }\n  \n  // 3. Fill the rest with zeros\n  while (col.length &lt; 4) {\n    col.unshift(0);\n  }\n  \n  // 4. Update the grid and check if moved\n  for (let r = 0; r &lt; 4; r++) {\n    if (this.grid[r][c] !== col[r]) {\n      moved = true;\n    }\n    this.grid[r][c] = col[r];\n  }\n}\n\nreturn moved;\n}\n// Make a move in the specified direction move(direction) { if (this.won || this.lost) { return false; }\nlet moved = false;\n\nswitch (direction) {\n  case 'left':\n    moved = this.moveLeft();\n    break;\n  case 'right':\n    moved = this.moveRight();\n    break;\n  case 'up':\n    moved = this.moveUp();\n    break;\n  case 'down':\n    moved = this.moveDown();\n    break;\n}\n\nif (moved) {\n  this.addRandomTile();\n  this.checkWin();\n  \n  if (this.isGameOver()) {\n    this.lost = true;\n  }\n}\n\nreturn moved;\n}\n// Get the current game state getState() { return { grid: this.grid, score: this.score, won: this.won, lost: this.lost, emptyCells: this.getEmptyCells().length }; } }\n// Example usage: // // const game = new Game2048(); // // // Handle keyboard input // document.addEventListener(‘keydown’, (event) =&gt; { // switch(event.key) { // case ‘ArrowLeft’: // game.move(‘left’); // break; // case ‘ArrowRight’: // game.move(‘right’); // break; // case ‘ArrowUp’: // game.move(‘up’); // break; // case ‘ArrowDown’: // game.move(‘down’); // break; // } //\n// // Update UI based on game.getState() // });\n\n\n\nI’ll create the core logic for the 2048 game. Let me implement that for you:\nThis JavaScript implementation provides the complete logic for the 2048 game. The key aspects of the implementation include:\n\nBoard Management:\n\n4x4 grid represented as a 2D array\nInitialization with two random tiles (90% chance of 2, 10% chance of 4)\n\nGame Mechanics:\n\nMovement in four directions (up, right, down, left)\nMerging identical tiles during movement\nScore tracking when tiles merge\nWin detection when a 2048 tile is created\nLoss detection when no moves are possible\n\nGame Flow:\n\nAfter each valid move, a new random tile appears\nThe game ends when either 2048 is reached (win) or no more moves are possible (loss)\n\n\nTo use this code, you would create a new game instance and call the move method with the direction (“up”, “right”, “down”, or “left”). The getState method returns the current state of the game including the grid, score, and game status.\nWould you like me to explain any specific part of the implementation in more detail?\nBelow is the code implementation\nclass Game2048 { constructor(size = 4) { this.size = size; this.grid = Array(size).fill().map(() =&gt; Array(size).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Initialize with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n}\n// Add a new random tile (2 or 4) to an empty cell addRandomTile() { if (this.isGridFull()) return false;\nlet emptyCells = [];\n\n// Find all empty cells\nfor (let row = 0; row &lt; this.size; row++) {\n  for (let col = 0; col &lt; this.size; col++) {\n    if (this.grid[row][col] === 0) {\n      emptyCells.push({ row, col });\n    }\n  }\n}\n\n// If there are empty cells, add a new tile\nif (emptyCells.length &gt; 0) {\n  const { row, col } = emptyCells[Math.floor(Math.random() * emptyCells.length)];\n  // 90% chance of 2, 10% chance of 4\n  this.grid[row][col] = Math.random() &lt; 0.9 ? 2 : 4;\n  return true;\n}\n\nreturn false;\n}\n// Check if the grid is completely filled isGridFull() { for (let row = 0; row &lt; this.size; row++) { for (let col = 0; col &lt; this.size; col++) { if (this.grid[row][col] === 0) { return false; } } } return true; }\n// Check if any moves are possible canMove() { // Check for empty cells if (!this.isGridFull()) return true;\n// Check for possible merges horizontally and vertically\nfor (let row = 0; row &lt; this.size; row++) {\n  for (let col = 0; col &lt; this.size; col++) {\n    const value = this.grid[row][col];\n    \n    // Check right\n    if (col &lt; this.size - 1 && value === this.grid[row][col + 1]) {\n      return true;\n    }\n    \n    // Check down\n    if (row &lt; this.size - 1 && value === this.grid[row + 1][col]) {\n      return true;\n    }\n  }\n}\n\nreturn false;\n}\n// Move tiles in a specific direction and merge where possible move(direction) { if (this.won || this.lost) return false;\nlet moved = false;\nlet prevGrid = JSON.parse(JSON.stringify(this.grid));\n\nswitch(direction) {\n  case 'up':\n    moved = this.moveUp();\n    break;\n  case 'right':\n    moved = this.moveRight();\n    break;\n  case 'down':\n    moved = this.moveDown();\n    break;\n  case 'left':\n    moved = this.moveLeft();\n    break;\n  default:\n    return false;\n}\n\n// If the grid changed, add a new random tile\nif (moved) {\n  this.addRandomTile();\n  \n  // Check if the player won (reached 2048)\n  this.checkWin();\n  \n  // Check if the player lost (no moves available)\n  if (!this.canMove()) {\n    this.lost = true;\n  }\n  \n  return true;\n}\n\nreturn false;\n}\n// Helper function to rotate the grid for easier move implementation rotateGrid(times = 1) { let newGrid = JSON.parse(JSON.stringify(this.grid));\nfor (let t = 0; t &lt; times; t++) {\n  const rotated = Array(this.size).fill().map(() =&gt; Array(this.size).fill(0));\n  \n  for (let row = 0; row &lt; this.size; row++) {\n    for (let col = 0; col &lt; this.size; col++) {\n      rotated[col][this.size - 1 - row] = newGrid[row][col];\n    }\n  }\n  \n  newGrid = rotated;\n}\n\nreturn newGrid;\n}\n// Move tiles left and merge where possible moveLeft() { let moved = false;\nfor (let row = 0; row &lt; this.size; row++) {\n  // Get non-zero tiles in this row\n  let tiles = this.grid[row].filter(val =&gt; val !== 0);\n  \n  // Merge adjacent equal tiles\n  for (let i = 0; i &lt; tiles.length - 1; i++) {\n    if (tiles[i] === tiles[i + 1]) {\n      tiles[i] *= 2;\n      tiles[i + 1] = 0;\n      this.score += tiles[i];\n      moved = true;\n      i++; // Skip the merged tile\n    }\n  }\n  \n  // Remove zeros (merged tiles)\n  tiles = tiles.filter(val =&gt; val !== 0);\n  \n  // Fill in remaining cells with zeros\n  while (tiles.length &lt; this.size) {\n    tiles.push(0);\n  }\n  \n  // Check if the row changed\n  if (!this.grid[row].every((val, idx) =&gt; val === tiles[idx])) {\n    moved = true;\n  }\n  \n  // Update the row\n  this.grid[row] = tiles;\n}\n\nreturn moved;\n}\n// Move tiles right by reversing, moving left, and reversing back moveRight() { for (let row = 0; row &lt; this.size; row++) { this.grid[row].reverse(); }\nconst moved = this.moveLeft();\n\nfor (let row = 0; row &lt; this.size; row++) {\n  this.grid[row].reverse();\n}\n\nreturn moved;\n}\n// Move tiles up by transposing, moving left, and transposing back moveUp() { // Transpose this.grid = this.rotateGrid(3);\nconst moved = this.moveLeft();\n\n// Transpose back\nthis.grid = this.rotateGrid();\n\nreturn moved;\n}\n// Move tiles down by transposing, moving right, and transposing back moveDown() { // Transpose this.grid = this.rotateGrid(3);\nconst moved = this.moveRight();\n\n// Transpose back\nthis.grid = this.rotateGrid();\n\nreturn moved;\n}\n// Check if the player has won (reached 2048) checkWin() { for (let row = 0; row &lt; this.size; row++) { for (let col = 0; col &lt; this.size; col++) { if (this.grid[row][col] === 2048) { this.won = true; return true; } } } return false; }\n// Get the current state of the game getState() { return { grid: this.grid, score: this.score, won: this.won, lost: this.lost, canMove: this.canMove() }; }\n// Reset the game reset() { this.grid = Array(this.size).fill().map(() =&gt; Array(this.size).fill(0)); this.score = 0; this.won = false; this.lost = false;\n// Initialize with two random tiles\nthis.addRandomTile();\nthis.addRandomTile();\n} }\n// Example usage: // const game = new Game2048(); // game.move(‘left’); // game.move(‘right’); // game.move(‘up’); // game.move(‘down’); // console.log(game.getState());\n\n\n\nWhile on the surface the two outputs look very similar we can see that the code in the extended version is better commented and goes into greater detail. Perhaps this was not the best test case to see the differences in the models - it will be interesting to see how this gap grows and what types of models are used for everyday tasks."
  },
  {
    "objectID": "posts/003_Human_Test/index.html",
    "href": "posts/003_Human_Test/index.html",
    "title": "Humanity’s Last Exam Overview",
    "section": "",
    "text": "Benchmarking for LLMs is like standarardized testing to compare models. By having models do the same set of tasks or answer the same set of questions, we can see how they perform against eachother. The idea of benchmarking is placing a relative place for compaision and exists in almost all disciplines.\nThe benchmark that we will look at today, Humanity’s Last Exam (HLE), is an exam comprised of 3,000 difficult out of context questions from a variety of fields including, mathematics, humanities, and the natural sciences. The exam is multimodel - with some questions having images and others being purely text-based.\nFor context of the difficult of the exam see the below image of various model’s perfomance on the HLE benchmark:\n\n\n\nWe can see that no model is performing well on the bench mark (with the max being a 13 percent accuracy). Knowing this we can test some of these sample questions on GPT to see how it performs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should note that the answers are not provided to these questions (as this would potentially leak them to be trained by an LLM) so for now we will have to use guess ourselves if this is infact the correct response (likely not as we can see above). This benchmark will serve as a good indicator for the progress of LLMs. That is to say that if a model is able to perform well on such an assessment that it has been exposed to such a wide range of material that it is able to answer even the most obscurley difficult domain specific questions correctly. For some, HLE serves as the final indicator of AGI, while for others it is just another in a long line of benchmarks."
  },
  {
    "objectID": "posts/003_Human_Test/index.html#benchmarking",
    "href": "posts/003_Human_Test/index.html#benchmarking",
    "title": "Humanity’s Last Exam Overview",
    "section": "",
    "text": "Benchmarking for LLMs is like standarardized testing to compare models. By having models do the same set of tasks or answer the same set of questions, we can see how they perform against eachother. The idea of benchmarking is placing a relative place for compaision and exists in almost all disciplines.\nThe benchmark that we will look at today, Humanity’s Last Exam (HLE), is an exam comprised of 3,000 difficult out of context questions from a variety of fields including, mathematics, humanities, and the natural sciences. The exam is multimodel - with some questions having images and others being purely text-based.\nFor context of the difficult of the exam see the below image of various model’s perfomance on the HLE benchmark:\n\n\n\nWe can see that no model is performing well on the bench mark (with the max being a 13 percent accuracy). Knowing this we can test some of these sample questions on GPT to see how it performs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should note that the answers are not provided to these questions (as this would potentially leak them to be trained by an LLM) so for now we will have to use guess ourselves if this is infact the correct response (likely not as we can see above). This benchmark will serve as a good indicator for the progress of LLMs. That is to say that if a model is able to perform well on such an assessment that it has been exposed to such a wide range of material that it is able to answer even the most obscurley difficult domain specific questions correctly. For some, HLE serves as the final indicator of AGI, while for others it is just another in a long line of benchmarks."
  },
  {
    "objectID": "posts/007_Puzzles/index.html",
    "href": "posts/007_Puzzles/index.html",
    "title": "GPT Solving Puzzles",
    "section": "",
    "text": "I’ve always loved solving brain teasers and puzzles - I recently came across this arcticle with some classin puzzles to solve:https://puzzlewocky.com/brain-teasers/classic-short-brain-teasers-and-riddles/\nI know that LLMs struggle with this type of reasoning and natural language parsing - so I figured that it would be interesting to see how GPT performs\n\n\nQuestion: Leslie, Eloise, Bill and Bob belong to an exclusive club. What is it?\nGPT Response: They are all names from the NATO phonetic alphabet: - Leslie → Lima - Eloise → Echo - Bill → Bravo - Bob → Bravo They belong to the “exclusive club” of names found in the phonetic alphabet!\nAnswer: The club of people whose names can be spelled upside down with a calculator: 317537, 351073, 7718, and 808, respectively.\nAnalysis: GPT got this one wrong - but had a very creative response, bringing up the NATO phonetic alphabet.\n\n\n\nQuestion: You have a bucket containing one gallon of water and a bucket containing one gallon of wine. You fill a one-cup measuring cup with wine and pour it into the water bucket. You then fill the measuring cup with one cup of the water-wine mixture and pour it back into the wine bucket. At that point, is there more water in the wine, or more wine in the water?\nGPT Response: The key to solving this is realizing that after the second transfer, the total volume in each bucket remains the same—one gallon in each. However, the amount of wine that leaves the water bucket during the second transfer is proportional to the dilution caused by the first transfer.\nStep 1: First Transfer (Wine to Water) Start with 1 gallon (16 cups) of wine and 1 gallon (16 cups) of water. Transfer 1 cup of wine to the water bucket. Now, the water bucket has 15 cups of water + 1 cup of wine (total: 16 cups), and the wine bucket has 15 cups of wine.\nStep 2: Second Transfer (Water-Wine Mix to Wine Bucket) The water bucket contains a 1:15 wine-to-water ratio (1 wine cup in 16 total). When you take 1 cup from it, the portion taken consists of 15/16 cup water and 1/16 cup wine. Adding this back to the wine bucket, it now contains: 15 + (1/16) = 15 1/16 cups of wine 0 + (15/16) = 15/16 cups of water\nComparison: Water in Wine vs. Wine in Water The water bucket contains 1 cup of wine. The wine bucket contains 15/16 cup of water. Thus, there is more wine in the water bucket than water in the wine bucket.\nAnswer: The proportions are exactly the same. Because you started with one gallon of water and one gallon of wine, and you now have two one-gallon mixtures, the amount of water that has been transferred to the wine bucket must have been replaced with an equal amount of wine, and vice versa.\nAnalysis: Here despite a lengthy calculation - GPT again got it wrong. I think that if it takes a purely mathmatical approach to these types of questions it will inevitably fail in response.\n\n\n\nQuestion: How can 8 + 8 = 4 ?\nGPT Response: If you’re working with a 12-hour clock, adding 8 hours to 8 AM lands you at 4 PM.\nAnswer: 8 a.m. plus 8 hours equals 4 p.m.\nAnalysis: GPT got this question correct!\n\n\n\nQuestion: A hostess has prepared a delicious dinner for eight people. However, a ninth person arrives unexpectedly. There is plenty of food, but only eight chairs. It seems someone will have to stand, until the hostess arrives at an ingenious solution. She seats the first guest and has his girlfriend sit on his lap for a minute. Then the seats the third guest in the second chair, the fourth guest in the third chair, and so on, all around the table, until the eighth guest sits in the seventh chair. Then she simply has the ninth guest get off her boyfriend’s lap and take the eighth chair. Nine guests in eight chairs! At least that’s how she describes it to you. Can you see where the hostess misled you?\nGPT Response: The hostess’s explanation is a clever trick in wording, but it doesn’t actually work. The key flaw in her reasoning is that at the beginning, the first guest’s girlfriend was already using a seat—she was just temporarily sitting on his lap.\nLet’s break it down step by step:\n\nThe first guest sits in chair #1.\nHis girlfriend sits on his lap (not in a new chair).\nThe third guest sits in chair #2, the fourth in chair #3, and so on.\nBy the time the eighth guest sits in chair #7, all seven of the available chairs (besides the one originally used for the lap-sitting trick) are occupied.\nNow, the ninth guest is supposed to take chair #8, but where did it come from?\n\nThe trick is that the hostess made it seem like the ninth guest was “freed up” when the girlfriend stood up, but in reality, she was never taking up a chair in the first place! The total number of seated guests at any given time never exceeds the number of chairs.\nThe illusion works by making you think that temporarily placing someone on a lap somehow created an extra chair, but it didn’t—it was just a distraction!\nAnswer: The girlfriend was not the ninth guest; she was the second. The ninth guest is still standing.\nAnalysis: GPT again was able to answer this question correctly!\n\n\n\nQuestion: A wealthy matriarch has passed away, leaving a valuable collection of 13 antique vehicles. Her will states that her first-born child will receive half of her collection, her second-born will receive one-third, and her third child will receive one-sixth. The cars may not be sold, but must be divided among the heirs, and if they cannot follow the terms of the will, they get nothing. The three heirs cannot figure out how to split up the vehicles, so they consult with a wise counselor, who comes up with an elegant solution. What does the counselor propose?\nGPT Response: The problem seems impossible at first because:\nThe first-born is supposed to receive ½ of 13 = 6.5 cars The second-born is supposed to receive ⅓ of 13 = 4.33 cars The third-born is supposed to receive ⅙ of 13 = 2.17 cars Since the heirs can’t physically split a car and they aren’t allowed to sell them, they are stuck—until the wise counselor steps in with an elegant trick.\nThe Counselor’s Solution: The counselor lends them one additional car, temporarily increasing the collection to 14 cars. Now, dividing 14 among the heirs per the will: First-born gets ½ of 14 = 7 cars Second-born gets ⅓ of 14 = 4 cars Third-born gets ⅙ of 14 = 2 cars The total distributed: 7 + 4 + 2 = 13 cars.\nThe extra car (which the counselor lent them) is still there, untouched, so the counselor simply takes it back. This clever trick works because 14 is a common multiple of 2, 3, and 6, making the division possible, while still leaving the original 13 cars correctly distributed.\nAnswer: The counselor takes one vehicle as her fee. The remaining 12 are therefore easy to split up: 6 for the first-born, 4 for the second, and 2 for the third.\nAnalysis: Frankly - both of these answers seem a bit far fetched with the correct answer making slightly more sense than GPT, however it would seem that GPT made a mistake as 14/3 is not 4. That being said I am not sure if this problem has an official correct answer as there seems to be odd rules that can be added to the bargaining"
  },
  {
    "objectID": "posts/007_Puzzles/index.html#solving-puzzles",
    "href": "posts/007_Puzzles/index.html#solving-puzzles",
    "title": "GPT Solving Puzzles",
    "section": "",
    "text": "I’ve always loved solving brain teasers and puzzles - I recently came across this arcticle with some classin puzzles to solve:https://puzzlewocky.com/brain-teasers/classic-short-brain-teasers-and-riddles/\nI know that LLMs struggle with this type of reasoning and natural language parsing - so I figured that it would be interesting to see how GPT performs\n\n\nQuestion: Leslie, Eloise, Bill and Bob belong to an exclusive club. What is it?\nGPT Response: They are all names from the NATO phonetic alphabet: - Leslie → Lima - Eloise → Echo - Bill → Bravo - Bob → Bravo They belong to the “exclusive club” of names found in the phonetic alphabet!\nAnswer: The club of people whose names can be spelled upside down with a calculator: 317537, 351073, 7718, and 808, respectively.\nAnalysis: GPT got this one wrong - but had a very creative response, bringing up the NATO phonetic alphabet.\n\n\n\nQuestion: You have a bucket containing one gallon of water and a bucket containing one gallon of wine. You fill a one-cup measuring cup with wine and pour it into the water bucket. You then fill the measuring cup with one cup of the water-wine mixture and pour it back into the wine bucket. At that point, is there more water in the wine, or more wine in the water?\nGPT Response: The key to solving this is realizing that after the second transfer, the total volume in each bucket remains the same—one gallon in each. However, the amount of wine that leaves the water bucket during the second transfer is proportional to the dilution caused by the first transfer.\nStep 1: First Transfer (Wine to Water) Start with 1 gallon (16 cups) of wine and 1 gallon (16 cups) of water. Transfer 1 cup of wine to the water bucket. Now, the water bucket has 15 cups of water + 1 cup of wine (total: 16 cups), and the wine bucket has 15 cups of wine.\nStep 2: Second Transfer (Water-Wine Mix to Wine Bucket) The water bucket contains a 1:15 wine-to-water ratio (1 wine cup in 16 total). When you take 1 cup from it, the portion taken consists of 15/16 cup water and 1/16 cup wine. Adding this back to the wine bucket, it now contains: 15 + (1/16) = 15 1/16 cups of wine 0 + (15/16) = 15/16 cups of water\nComparison: Water in Wine vs. Wine in Water The water bucket contains 1 cup of wine. The wine bucket contains 15/16 cup of water. Thus, there is more wine in the water bucket than water in the wine bucket.\nAnswer: The proportions are exactly the same. Because you started with one gallon of water and one gallon of wine, and you now have two one-gallon mixtures, the amount of water that has been transferred to the wine bucket must have been replaced with an equal amount of wine, and vice versa.\nAnalysis: Here despite a lengthy calculation - GPT again got it wrong. I think that if it takes a purely mathmatical approach to these types of questions it will inevitably fail in response.\n\n\n\nQuestion: How can 8 + 8 = 4 ?\nGPT Response: If you’re working with a 12-hour clock, adding 8 hours to 8 AM lands you at 4 PM.\nAnswer: 8 a.m. plus 8 hours equals 4 p.m.\nAnalysis: GPT got this question correct!\n\n\n\nQuestion: A hostess has prepared a delicious dinner for eight people. However, a ninth person arrives unexpectedly. There is plenty of food, but only eight chairs. It seems someone will have to stand, until the hostess arrives at an ingenious solution. She seats the first guest and has his girlfriend sit on his lap for a minute. Then the seats the third guest in the second chair, the fourth guest in the third chair, and so on, all around the table, until the eighth guest sits in the seventh chair. Then she simply has the ninth guest get off her boyfriend’s lap and take the eighth chair. Nine guests in eight chairs! At least that’s how she describes it to you. Can you see where the hostess misled you?\nGPT Response: The hostess’s explanation is a clever trick in wording, but it doesn’t actually work. The key flaw in her reasoning is that at the beginning, the first guest’s girlfriend was already using a seat—she was just temporarily sitting on his lap.\nLet’s break it down step by step:\n\nThe first guest sits in chair #1.\nHis girlfriend sits on his lap (not in a new chair).\nThe third guest sits in chair #2, the fourth in chair #3, and so on.\nBy the time the eighth guest sits in chair #7, all seven of the available chairs (besides the one originally used for the lap-sitting trick) are occupied.\nNow, the ninth guest is supposed to take chair #8, but where did it come from?\n\nThe trick is that the hostess made it seem like the ninth guest was “freed up” when the girlfriend stood up, but in reality, she was never taking up a chair in the first place! The total number of seated guests at any given time never exceeds the number of chairs.\nThe illusion works by making you think that temporarily placing someone on a lap somehow created an extra chair, but it didn’t—it was just a distraction!\nAnswer: The girlfriend was not the ninth guest; she was the second. The ninth guest is still standing.\nAnalysis: GPT again was able to answer this question correctly!\n\n\n\nQuestion: A wealthy matriarch has passed away, leaving a valuable collection of 13 antique vehicles. Her will states that her first-born child will receive half of her collection, her second-born will receive one-third, and her third child will receive one-sixth. The cars may not be sold, but must be divided among the heirs, and if they cannot follow the terms of the will, they get nothing. The three heirs cannot figure out how to split up the vehicles, so they consult with a wise counselor, who comes up with an elegant solution. What does the counselor propose?\nGPT Response: The problem seems impossible at first because:\nThe first-born is supposed to receive ½ of 13 = 6.5 cars The second-born is supposed to receive ⅓ of 13 = 4.33 cars The third-born is supposed to receive ⅙ of 13 = 2.17 cars Since the heirs can’t physically split a car and they aren’t allowed to sell them, they are stuck—until the wise counselor steps in with an elegant trick.\nThe Counselor’s Solution: The counselor lends them one additional car, temporarily increasing the collection to 14 cars. Now, dividing 14 among the heirs per the will: First-born gets ½ of 14 = 7 cars Second-born gets ⅓ of 14 = 4 cars Third-born gets ⅙ of 14 = 2 cars The total distributed: 7 + 4 + 2 = 13 cars.\nThe extra car (which the counselor lent them) is still there, untouched, so the counselor simply takes it back. This clever trick works because 14 is a common multiple of 2, 3, and 6, making the division possible, while still leaving the original 13 cars correctly distributed.\nAnswer: The counselor takes one vehicle as her fee. The remaining 12 are therefore easy to split up: 6 for the first-born, 4 for the second, and 2 for the third.\nAnalysis: Frankly - both of these answers seem a bit far fetched with the correct answer making slightly more sense than GPT, however it would seem that GPT made a mistake as 14/3 is not 4. That being said I am not sure if this problem has an official correct answer as there seems to be odd rules that can be added to the bargaining"
  },
  {
    "objectID": "posts/007_Puzzles/index.html#conclusion",
    "href": "posts/007_Puzzles/index.html#conclusion",
    "title": "GPT Solving Puzzles",
    "section": "Conclusion",
    "text": "Conclusion\nOverall GPT performed fine on these brain teasers - getting some right and others wrong. It is important to recognize that there is a chance that GPT was trained on some of these correct repsonses and would be able to come up with the correct answer right away. It will be interesting to see how LLMs progress against these types of problems as a benchmark."
  }
]