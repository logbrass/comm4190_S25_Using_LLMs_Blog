{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Adversarial Prompts\"\n",
    "description: \"How does Claude Sonnet 3.7 perform with tough to parse prompts\"\n",
    "author: \"Logan Brassington\"\n",
    "date: \"5/9/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58618eb-dd92-483b-a802-cf2083e61d6d",
   "metadata": {},
   "source": [
    "<img src=\"not.png\" width=\"50%\"/>\n",
    "\n",
    "## Adversarial Prompts\n",
    "\n",
    "Adversarial prompts are inputs intentionally designed to confuse an LLM - often by coming up with parsing issues. These prompts stress-test a modelâ€™s reasoning. It is important to test this as typos will happen with any user input - as will odd charaters.\n",
    "\n",
    "The below prompts are designed to push Sonnet 3.7 to failure. All of the prompts will be prefaced with \"What does this say:\".\n",
    "\n",
    "### Whitespace Flooding\n",
    "\n",
    "**Prompt:** W h a t i s t h e c a p i t a l o f F r a n c e ?\n",
    "\n",
    "**CLAUDE:** \n",
    "The text says: \"What is the capital of France?\"\n",
    "- The capital of France is Paris. Paris has been the capital city of France since 987 CE when Hugh Capet, the first king of the Capetian dynasty, made the city his seat of government.\n",
    "\n",
    "### Zalgo Text\n",
    "\n",
    "**Prompt:** WÌ´Í‘Í„Ì©Ì»hÌµÍ˜Ì‡ÍšaÌ¸Ì…ÌºÍ”tÌ´ÍÍ‰ ÌµÌ¿Ì¼iÌ¸Ì”Í•ÍšsÌ¶Í€Ì“ÍÌ¡ Ì·ÌÍ…gÌ´ÌÍ˜Ì©oÌµÍÌ ÍiÌ·ÌšÌ—Í“nÌ´Ì†ÌšÍ‡Í…gÌ¶ÍƒÌ½Ì˜Ìº Ì·Ì„Í„Ì¹oÌ´Í€ÌœnÌ¸Í„Ì ?\n",
    "\n",
    "**CLAUDE:**\n",
    "The text says: \"What is going on?\"\n",
    "\n",
    "This appears to be regular text that has been stylized with special characters or \"zalgo text\" formatting, which adds diacritical marks above and below the letters to create a distorted, glitchy appearance. It's a style sometimes used online for aesthetic purposes or to convey a sense of distortion or chaos.\n",
    "\n",
    "### ASCII Substitution\n",
    "\n",
    "**Prompt:** Wh@t d03s â€œc0gn1t1v3 sc13nc3â€ m34n?\n",
    "\n",
    "**CLAUDE:**\n",
    "The text says: \"What does 'cognitive science' mean?\"\n",
    "\n",
    "The message uses \"leetspeak\" or \"1337speak,\" a form of internet slang where characters are replaced with numbers or symbols that visually resemble them (like @ for a, 3 for e, 1 for i, etc.).\n",
    "\n",
    "Cognitive science is an interdisciplinary field that studies the mind and its processes. It combines research methods from psychology, neuroscience, linguistics, philosophy, computer science, and anthropology to understand how the human mind works, including perception, thinking, learning, language, attention, memory, reasoning, and emotion. The field aims to explain the nature of human knowledge, its components, development, and use.\n",
    "\n",
    "### Broken Case & Punctuation\n",
    "\n",
    "**Prompt:** wHy IS, tHe EaRTh sO cOld In thE NigHT?\n",
    "\n",
    "**CLAUDE:**\n",
    "The text says: \"Why is the earth so cold in the night?\"\n",
    "\n",
    "This text is written in \"mocking SpongeBob\" or \"alternating case\" style, where letters alternate between uppercase and lowercase in an irregular pattern. This style is often used online to convey a sarcastic or mocking tone.\n",
    "\n",
    "As for the question itself: The Earth gets colder at night because it no longer receives warmth from the sun's radiation. During the day, the Earth's surface absorbs solar energy and heats up. When the sun sets, this heat source is removed, and the Earth's surface continues to radiate heat into space, causing temperatures to drop. This is especially noticeable in areas with dry air (like deserts), as water vapor in the atmosphere can help retain some heat overnight.\n",
    "\n",
    "### Delimiter Injection\n",
    "\n",
    "**Prompt:** ||What|||||is||||the||speed|of|light?\n",
    "\n",
    "**CLAUDE:**\n",
    "The text says: \"What is the speed of light?\"\n",
    "\n",
    "The message has added vertical bars (|) between and around some of the words, which don't change the meaning but create a visual separation between the words.\n",
    "\n",
    "The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second).\n",
    "\n",
    "### Base64-Like Encoded Prompt\n",
    "This is â€œWhat does the Moon do every night?â€ in Base64\n",
    "\n",
    "**Prompt:** V2hhdCBkb2VzIHRoZSBNb29uIGRvIGV2ZXJ5IG5pZ2h0Pw==\n",
    "\n",
    "**CLAUDE:**\n",
    "The text appears to be Base64 encoded. When decoded, it says: \"What does the Moon do every night?\"\n",
    "The Moon doesn't actually do the same thing \"every night\" as its behavior follows a monthly cycle rather than a daily one. \n",
    "\n",
    "### Broken Grammar from OCR-like Error\n",
    "\n",
    "**Prompt:** Whâ‚¬n i5 thâ‚¬ bâ‚¬5t tim3 t0 u53 AI in dâ‚¬cisi0n m@king?\n",
    "\n",
    "**CLAUDE:**\n",
    "This text says: \"When is the best time to use AI in decision making?\"\n",
    "The message uses leetspeak or textspeak substitutions (numbers and symbols for letters), including â‚¬ for e, 5 for s, 3 for e, 0 for o, and @ for a.\n",
    "\n",
    "### Bilingual Noise Prompt\n",
    "Prompt includes multiple languages interspersed with syntax glitches.\n",
    "\n",
    "**Prompt:** Â¿C0u1d ğŸ§ ä½  t3ll me â° lâ€™hist0ire oF ğŸPython?\n",
    "\n",
    "**CLAUDE:**\n",
    "The text says: \"Could you tell me the history of Python?\"\n",
    "\n",
    "## Conclusion:\n",
    "Overall Claude crushed this - I was surprsied that the Base64 did not trick it in the slightest. This likely means that there is an advanced parser running in the background to acompany the LLM. These prompts are a great example about how LLMs will continue to differentiate the signal from the noise and pickout the relevant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38860545-ebfd-4a0b-9f26-9f0659cc6e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
