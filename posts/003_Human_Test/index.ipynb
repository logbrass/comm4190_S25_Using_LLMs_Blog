{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Humanity's Last Exam Overview\"\n",
    "description: \"An overview of the HLE Benchmark and how GPT 4o does on a few questions\"\n",
    "author: \"Logan Brassington\"\n",
    "date: \"2/6/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Benchmarking\n",
    "  - Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58618eb-dd92-483b-a802-cf2083e61d6d",
   "metadata": {},
   "source": [
    "<img src=\"bo.png\" width=\"50%\"/>\n",
    "\n",
    "## Benchmarking\n",
    "\n",
    "\n",
    "\n",
    "Benchmarking for LLMs is like standarardized testing to compare models. By having models do the same set of tasks or answer the same set of questions, we can see how they perform against eachother. The idea of benchmarking is placing a relative place for compaision and exists in almost all disciplines.\n",
    "\n",
    "The benchmark that we will look at today, **Humanity's Last Exam (HLE)**, is an exam comprised of 3,000 difficult out of context questions from a variety of fields including, mathematics, humanities, and the natural sciences. The exam is multimodel - with some questions having images and others being purely text-based.\n",
    "\n",
    "For context of the difficult of the exam see the below image of various model's perfomance on the HLE benchmark:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7e97a-f92e-411d-8c62-1608d7820af7",
   "metadata": {},
   "source": [
    "\n",
    "### Model Perfomance on HLE\n",
    "\n",
    "<img src=\"HLE.png\" width=\"50%\"/>\n",
    "\n",
    "We can see that no model is performing well on the bench mark (with the max being a 13 percent accuracy). Knowing this we can test some of these sample questions on GPT to see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226712ea-5b8c-4324-afbc-fa68820f5a75",
   "metadata": {},
   "source": [
    "### Question - Ecology:\n",
    "\n",
    "<img src=\"img12.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fe6da-d7d5-439e-b38c-826d6f9e9dba",
   "metadata": {},
   "source": [
    "### Question - Greek Mythology:\n",
    "\n",
    "<img src=\"img13.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c65be-05ea-4895-b4c3-7ca888d04cfd",
   "metadata": {},
   "source": [
    "### Question - Roman Translation:\n",
    "\n",
    "<img src=\"img14.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3624a-ed7a-40db-b118-24e7038f8f6d",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "\n",
    "We should note that the answers are not provided to these questions (as this would potentially leak them to be trained by an LLM) so for now we will have to use guess ourselves if this is infact the correct response (likely not as we can see above). This benchmark will serve as\n",
    "a good indicator for the progress of LLMs. That is to say that if a model is able to perform well on such an assessment that it has been exposed to such a wide range of material that it is able to answer even the most obscurley difficult domain specific questions correctly.\n",
    "For some, HLE serves as the final indicator of AGI, while for others it is just another in a long line of benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8d050-f12b-41e6-bc74-ef171a523962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
